{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd4c1b3-8076-4b34-a584-d7ad163ce102",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import notebookutils\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "\n",
    "from deltalake import write_deltalake, DeltaTable\n",
    "from sempy.fabric import get_notebook_workspace_id, list_workspaces  \n",
    "from zoneinfo import ZoneInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29eb050-36b6-42d5-9a3c-226a758656eb",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Building paths\n",
    "workspace_id = get_notebook_workspace_id()  \n",
    "workspace_name = list_workspaces().query(\"Id == @workspace_id\")['Name'].iloc[0]\n",
    "print(f'Workspace: {workspace_name} | ID: {workspace_id}')\n",
    "\n",
    "lakehouse_name = 'LK_Lakehouse'\n",
    "lakehouse_path = f'abfss://{workspace_name}@onelake.dfs.fabric.microsoft.com/{lakehouse_name}.Lakehouse'\n",
    "\n",
    "tables_path = f'{lakehouse_path}/Tables'\n",
    "print(f'Tables path: {tables_path}') \n",
    "\n",
    "landing_path = f'{lakehouse_path}/Files/Landing/Files'\n",
    "print(f'Files landing path: {landing_path}') \n",
    "\n",
    "loaded_path = f'{lakehouse_path}/Files/Loaded'\n",
    "print(f'Files loaded path: {loaded_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22c5ee7-7510-4d26-a1e3-2143990a1cdf",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters to expose in Data Pipeline  \n",
    "is_incremental = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfeca15-37c5-4e49-8be2-1bfb27286c6c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Load CSV files to Delta\n",
    "\n",
    "# Read files\n",
    "files = notebookutils.fs.ls(landing_path)\n",
    "\n",
    "if not files:\n",
    "    print(f'Any file found in {landing_path}') \n",
    "\n",
    "else:\n",
    "    files_qty = len(files)\n",
    "    print(f'Found {files_qty} files in {landing_path}')  \n",
    "\n",
    "    # Creates a empyt list to append each DataFrame\n",
    "    df_list = [] \n",
    "\n",
    "    # Iterate\n",
    "    for f in files:\n",
    "        file_name = f.name\n",
    "        if not file_name.endswith('.csv'):\n",
    "            continue\n",
    "        \n",
    "        date_str = file_name.split('_')[1].replace('.csv', '')        # Extract the date from the filename\n",
    "        df = pd.read_csv(f'{landing_path}/{file_name}')               # CSV to pandas.DataFrame\n",
    "        \n",
    "        df['Date'] = datetime.strptime(date_str, \"%Y-%m-%d\").date()   # Add the column 'Data'\n",
    "        \n",
    "        df_list.append(df)                                            # Append each DataFrame\n",
    "\n",
    "    # Concat the list in one DataFrame\n",
    "    df_final = pd.concat(df_list, ignore_index=True)                  \n",
    "\n",
    "    # Rename with friendly BI column names\n",
    "    df_final.rename(                                                  \n",
    "        columns={\n",
    "            'employee_id': 'EmployeeID',\n",
    "            'standard_hours': 'StandardHours',\n",
    "            'worked_hours': 'WorkedHours',\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    # Reordered columns\n",
    "    df_final = df_final[['Date', 'EmployeeID', 'StandardHours', 'WorkedHours']]\n",
    "\n",
    "    # Writing with mode\n",
    "    mode = 'append' if is_incremental else 'overwrite' \n",
    "    write_deltalake(f'{tables_path}/FactAbsenteeism', df_final, mode=mode)   \n",
    "    print(f'✓ CSV files were loaded to Delta tables with `{mode}` mode successfully!')\n",
    "\n",
    "    # Extracting max_date to incremental logic\n",
    "    max_date = df_final['Date'].max() \n",
    "    df_max_date = pd.DataFrame({'MaxDate': [max_date]}) \n",
    "    write_deltalake(f'{tables_path}/MaxDate', df_max_date, mode='overwrite')\n",
    "\n",
    "    # Getting datetime to build the path of loaded files\n",
    "    datetime_str = datetime.now(ZoneInfo('America/Sao_Paulo')).strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    loaded_path_with_date = f'{loaded_path}/{datetime_str}'\n",
    "\n",
    "    # Moving files landing from loading  \n",
    "    notebookutils.fs.mv(landing_path, loaded_path_with_date)   \n",
    "    print(f'✓ All files moved successfully!')\n",
    "    print(f'From: {landing_path}')\n",
    "    print(f'To: {loaded_path_with_date}/Files')  \n"
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "Jupyter",
   "name": "jupyter"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
